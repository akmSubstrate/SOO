{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SOO - Self Organizing Orrey: Testing Notebook\n",
    "#### created: 5/28/2025\n",
    "#### last edited: 5/28/2025\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "68a9c8912ad43c1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports + Logging Setup"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d002ba06b42b89fe"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%gui qt\n",
    "import numpy as np\n",
    "from typing import Callable, Optional\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "# Configure logging for the module\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)  # Set to INFO or DEBUG for more verbose output\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "try:\n",
    "    from mayavi import mlab\n",
    "except ImportError:\n",
    "    mlab = None\n",
    "    logger.warning(\"Mayavi is not installed. Visualization will be disabled.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-03T18:25:19.913110600Z",
     "start_time": "2025-06-03T18:25:15.931001100Z"
    }
   },
   "id": "b03e8559980db40"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SOOMap class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c568ee6063b013ab"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class SOOMap:\n",
    "    def __init__(self, num_nodes: int, input_dim: int, \n",
    "                 init_weight_scale: float = 0.01,\n",
    "                 init_coord_range: float = 1.0,\n",
    "                 learning_rate: float = 0.1, \n",
    "                 neighbor_radius: float = 0.5,\n",
    "                 distance_metric: Optional[Callable] = None,\n",
    "                 device: Optional[torch.device] = None):\n",
    "        \"\"\"\n",
    "        Initialize the Self-Organizing Orrery map.\n",
    "        \n",
    "        Parameters:\n",
    "        - num_nodes: Number of neurons/nodes in the map.\n",
    "        - input_dim: Dimensionality of input feature vectors.\n",
    "        - init_weight_scale: Scale (std-dev) for initial random weights (small Gaussian noise).\n",
    "        - init_coord_range: Range for initial coordinates (they will be uniform in [0, init_coord_range] in each dimension).\n",
    "        - learning_rate: Initial learning rate for weight updates.\n",
    "        - neighbor_radius: Initial neighborhood radius in coordinate space for coordinate updates.\n",
    "        - distance_metric: Function to compute distance between input and weights (defaults to Euclidean if None).\n",
    "        - device: Torch device to use (CPU or CUDA). If None, will use CPU.\n",
    "        \"\"\"\n",
    "        self.num_nodes = num_nodes\n",
    "        self.input_dim = input_dim\n",
    "        self.device = device if device is not None else torch.device(\"cpu\")\n",
    "        \n",
    "        # Initialize weight vectors (num_nodes x input_dim) with small Gaussian noise around 0\n",
    "        self.weights = torch.randn(num_nodes, input_dim, device=self.device) * init_weight_scale\n",
    "        # Initialize node coordinates (num_nodes x 3) uniformly in [0, init_coord_range]\n",
    "        self.coordinates = torch.rand(num_nodes, 3, device=self.device) * init_coord_range\n",
    "        \n",
    "        # Set initial learning parameters\n",
    "        self.initial_lr = learning_rate\n",
    "        self.current_lr = learning_rate\n",
    "        self.initial_radius = neighbor_radius\n",
    "        self.current_radius = neighbor_radius\n",
    "        \n",
    "        # Set distance metric for BMU (Euclidean by default)\n",
    "        if distance_metric is None:\n",
    "            # Euclidean distance: returns distance^2 for efficiency (no need for sqrt for comparison)\n",
    "            self.distance_metric = lambda w, x: torch.sum((w - x) ** 2, dim=1)\n",
    "        else:\n",
    "            self.distance_metric = distance_metric\n",
    "        \n",
    "        # Keep track of training progress (for example, last computed convergence value)\n",
    "        self.last_cv = 0.0\n",
    "        logger.info(f\"Initialized SOOMap with {num_nodes} nodes, input_dim={input_dim}, device={self.device}.\")\n",
    "    \n",
    "    def find_bmu(self, x: torch.Tensor) -> int:\n",
    "        \"\"\"\n",
    "        Find the Best Matching Unit (BMU) index for input vector x.\n",
    "        Uses the configured distance metric (Euclidean by default).\n",
    "        \"\"\"\n",
    "        # Ensure x is 1D (single sample)\n",
    "        if x.dim() != 1:\n",
    "            x = x.view(-1)\n",
    "        # Compute distance from x to all weights\n",
    "        dists = self.distance_metric(self.weights, x)\n",
    "        # BMU is the index of the minimum distance\n",
    "        bmu_idx = int(torch.argmin(dists).item())\n",
    "        return bmu_idx\n",
    "    \n",
    "    def update_weights(self, bmu_idx: int, x: torch.Tensor, lr: float):\n",
    "        \"\"\"\n",
    "        Update the weight vector of the BMU (and optionally neighbors) towards the input x using learning rate lr.\n",
    "        For simplicity, only the BMU's weight is updated in this MVP. (Neighbor weight updates can be added if desired.)\n",
    "        \"\"\"\n",
    "        # Calculate weight change for BMU\n",
    "        self.weights[bmu_idx] += lr * (x - self.weights[bmu_idx])\n",
    "        # (Optional extension: update neighbors' weights as well with a smaller factor.)\n",
    "    \n",
    "    def update_coordinates(self, bmu_idx: int, neighbor_indices: torch.Tensor, coord_lr: float):\n",
    "        \"\"\"\n",
    "        Update the coordinates of the BMU's neighbors (including BMU itself) towards the BMU's coordinates.\n",
    "        \n",
    "        Parameters:\n",
    "        - bmu_idx: Index of the BMU.\n",
    "        - neighbor_indices: Indices of neurons considered within the neighborhood radius (including BMU).\n",
    "        - coord_lr: Learning rate for coordinate updates.\n",
    "        \"\"\"\n",
    "        bmu_coord = self.coordinates[bmu_idx]\n",
    "        # Move each neighbor's coordinate a step closer to the BMU's coordinate\n",
    "        for ni in neighbor_indices:\n",
    "            # delta = fraction of the vector from neighbor to BMU\n",
    "            self.coordinates[ni] += coord_lr * (bmu_coord - self.coordinates[ni])\n",
    "        # (BMU itself will have no change if ni == bmu_idx because (bmu_coord - bmu_coord) = 0)\n",
    "    \n",
    "    def calc_convergence(self, data_ref: np.ndarray, significance: float = 0.05) -> float:\n",
    "        \"\"\"\n",
    "        Calculate a population-based convergence (embedding accuracy) measure.\n",
    "        Compares the distribution of neuron weights to the distribution of a reference subset of the input data.\n",
    "        \n",
    "        The method checks feature-wise if neurons and data appear to be drawn from the same distribution \n",
    "        by comparing their means and variances (a simplified two-sample test per feature).\n",
    "        \n",
    "        Returns:\n",
    "        - Convergence value between 0.0 and 1.0 indicating the fraction of features that are 'embedded' (matching in distribution).\n",
    "        \"\"\"\n",
    "        # Ensure reference data is a numpy array for easy mean/var calculation\n",
    "        data_ref = np.array(data_ref)\n",
    "        # Randomly select a set of neurons to compare (mix BMU neighborhoods and others)\n",
    "        # For simplicity, we'll use *all* neurons here for the distribution comparison in MVP.\n",
    "        neurons_sample = self.weights.detach().cpu().numpy()\n",
    "        \n",
    "        # Compute means and variances for each feature\n",
    "        data_means = data_ref.mean(axis=0)\n",
    "        data_vars = data_ref.var(axis=0, ddof=1)  # sample variance\n",
    "        neuron_means = neurons_sample.mean(axis=0)\n",
    "        neuron_vars = neurons_sample.var(axis=0, ddof=1)\n",
    "        n1 = data_ref.shape[0]    # number of data samples\n",
    "        n2 = neurons_sample.shape[0]  # number of neurons in sample\n",
    "        \n",
    "        # Feature embedding test: check if means and variances are similar for each feature\n",
    "        embedded_features = 0\n",
    "        for d in range(self.input_dim):\n",
    "            m1, m2 = data_means[d], neuron_means[d]\n",
    "            v1, v2 = data_vars[d], neuron_vars[d]\n",
    "            # Check means difference (using normal approximation)\n",
    "            mean_diff_ok = abs(m1 - m2) <= 1.96 * np.sqrt(v1/n1 + v2/n2)  # ~95% confidence interval for mean difference\n",
    "            # Check variance ratio (using a crude F-test approximation)\n",
    "            ratio = v1 / (v2 + 1e-9)\n",
    "            if ratio < 1:\n",
    "                ratio = 1.0 / (ratio + 1e-9)\n",
    "            var_diff_ok = ratio <= 2.0  # allow variance ratio within factor of 2 (roughly 95% for moderate sample sizes)\n",
    "            if mean_diff_ok and var_diff_ok:\n",
    "                embedded_features += 1\n",
    "        # Fraction of features that appear embedded\n",
    "        cv_value = embedded_features / float(self.input_dim)\n",
    "        self.last_cv = cv_value\n",
    "        return cv_value\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-03T18:25:19.928112Z",
     "start_time": "2025-06-03T18:25:19.913110600Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training Loop"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "25967eead2f28863"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class SOOMap(SOOMap):  # extend the previous class definition\n",
    "    def train(self, data: torch.Tensor, epochs: int = 100, batch_size: int = 1, \n",
    "              ref_sample_size: int = 100, min_lr: float = 0.01, min_radius: float = 0.01):\n",
    "        \"\"\"\n",
    "        Train the SOOMap on the given dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        - data: torch.Tensor of shape (n_samples, input_dim) containing the training data.\n",
    "        - epochs: Number of epochs to train for.\n",
    "        - batch_size: Number of samples to process at a time (1 = sequential stochastic updates).\n",
    "        - ref_sample_size: Size of reference subset for convergence calculation (if <= 0, use full data).\n",
    "        - min_lr: Minimum learning rate to which to decay.\n",
    "        - min_radius: Minimum neighborhood radius to which to decay.\n",
    "        \"\"\"\n",
    "        data = data.to(self.device)\n",
    "        n_samples = data.shape[0]\n",
    "        \n",
    "        # Prepare a fixed reference subset of data for convergence measurement\n",
    "        if ref_sample_size is None or ref_sample_size <= 0 or ref_sample_size > n_samples:\n",
    "            ref_data_np = data.cpu().numpy()\n",
    "        else:\n",
    "            # Randomly sample without replacement\n",
    "            indices = np.random.choice(n_samples, size=ref_sample_size, replace=False)\n",
    "            ref_data_np = data[indices].cpu().numpy()\n",
    "        \n",
    "        for epoch in range(1, epochs+1):\n",
    "            # Shuffle data indices for each epoch (stochastic training)\n",
    "            perm = torch.randperm(n_samples, device=self.device)\n",
    "            data_shuffled = data[perm]\n",
    "            \n",
    "            # Iterate over data in batches\n",
    "            for i in tqdm(range(0, n_samples, batch_size), desc=f\"Epoch {epoch}\", leave=False):\n",
    "                batch = data_shuffled[i:i+batch_size]\n",
    "                # Find BMUs for all samples in the batch\n",
    "                # (Compute distances in a vectorized manner for efficiency)\n",
    "                # batch shape: (batch_size, input_dim)\n",
    "                # weights shape: (num_nodes, input_dim)\n",
    "                # Use broadcasting to compute distance of each input to each weight\n",
    "                # (We will do it sample by sample to update sequentially for simplicity)\n",
    "                for x in batch:\n",
    "                    bmu_idx = self.find_bmu(x)\n",
    "                    # Determine neighbor nodes within current radius\n",
    "                    bmu_coord = self.coordinates[bmu_idx]\n",
    "                    # Compute squared distances from BMU to all nodes in coordinate space\n",
    "                    coord_dists = torch.sum((self.coordinates - bmu_coord) ** 2, dim=1)\n",
    "                    # Find indices of neighbors within radius^2\n",
    "                    neighbor_idx = torch.where(coord_dists <= (self.current_radius ** 2))[0]\n",
    "                    \n",
    "                    # Update weights and coordinates\n",
    "                    self.update_weights(bmu_idx, x, lr=self.current_lr)\n",
    "                    # For coordinate update, use a coordinate learning rate (could tie to current_lr or set separately)\n",
    "                    coord_lr = self.current_lr  # here we use same value for simplicity\n",
    "                    self.update_coordinates(bmu_idx, neighbor_idx, coord_lr=coord_lr)\n",
    "            \n",
    "            # At end of epoch, evaluate convergence on reference data\n",
    "            cv = self.calc_convergence(ref_data_np)\n",
    "            logger.info(f\"Epoch {epoch}: convergence (embedding) = {cv*100:.1f}%\")\n",
    "            \n",
    "            # Adjust learning rate and radius for next epoch (gradual decay towards minimum values)\n",
    "            # We use the convergence to modulate the decay: as cv approaches 1.0, lr and radius approach their minima.\n",
    "            # Simple schedule: lr_new = max(min_lr, initial_lr * (1 - cv)); similar for radius.\n",
    "            self.current_lr = max(min_lr, self.initial_lr * (1.0 - cv))\n",
    "            self.current_radius = max(min_radius, self.initial_radius * (1.0 - cv))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-03T18:25:19.945112700Z",
     "start_time": "2025-06-03T18:25:19.932112300Z"
    }
   },
   "id": "247ccb00a8b40a8c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualization code using Mayavi"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96e4d42b98943940"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Visualization code using Mayavi\n",
    "\n",
    "class SOOVisualizer:\n",
    "    def __init__(self, soo_map: SOOMap, save_frames: bool = False):\n",
    "        \"\"\"\n",
    "        Visualizer for Self-Organizing Orrery.\n",
    "        - soo_map: an instance of SOOMap.\n",
    "        - save_frames: whether to save each frame to file (frame_<epoch>.png) when updating.\n",
    "        \"\"\"\n",
    "        self.soo_map = soo_map\n",
    "        self.save_frames = save_frames\n",
    "        self.fig = None\n",
    "        self.points = None\n",
    "        # If Mayavi is available, set up the figure and initial scatter plot\n",
    "        if mlab:\n",
    "            # Use offscreen rendering if no GUI (optional)\n",
    "            mlab.options.offscreen = False  # Set True for headless saving without showing a window\n",
    "            self.fig = mlab.figure(size=(600, 600), bgcolor=(0.9, 0.9, 0.9))\n",
    "            # Plot initial node positions\n",
    "            coords = self.soo_map.coordinates.detach().cpu().numpy()\n",
    "            x, y, z = coords[:, 0], coords[:, 1], coords[:, 2]\n",
    "            # Use a scatter plot (points3d). All points same color for now; color them by cluster or weight later if needed.\n",
    "            self.points = mlab.points3d(x, y, z, scale_factor=0.05, color=(0, 0, 1))\n",
    "            mlab.view(azimuth=45, elevation=75, distance=5)  # set an initial camera view\n",
    "            mlab.title(\"SOO Nodes (Initial)\", size=0.4)\n",
    "    \n",
    "    def update(self, epoch: int = None):\n",
    "        \"\"\"Update the 3D scatter plot to the current coordinates of the SOO map's neurons.\"\"\"\n",
    "        if mlab is None or self.points is None:\n",
    "            return  # no-op if visualization is not available\n",
    "        coords = self.soo_map.coordinates.detach().cpu().numpy()\n",
    "        x, y, z = coords[:, 0], coords[:, 1], coords[:, 2]\n",
    "        # Update the points in the existing plot\n",
    "        self.points.mlab_source.set(x=x, y=y, z=z)\n",
    "        if epoch is not None:\n",
    "            mlab.title(f\"SOO Nodes (Epoch {epoch})\", size=0.4)\n",
    "        if self.save_frames and epoch is not None:\n",
    "            mlab.savefig(f\"frame_{epoch}.png\")\n",
    "    \n",
    "    def show(self):\n",
    "        \"\"\"Display the visualization window (blocks execution until closed).\"\"\"\n",
    "        if mlab:\n",
    "            mlab.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-03T18:25:19.960110800Z",
     "start_time": "2025-06-03T18:25:19.947111Z"
    }
   },
   "id": "8b250969cfe70809"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Synthetic Data Generator"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "965d918cfd533979"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def generate_blobs(num_samples: int = 500, num_features: int = 2, centers: int = 3, cluster_std: float = 0.1, random_state: int = None):\n",
    "    \"\"\"\n",
    "    Generate isotropic Gaussian blobs for clustering.\n",
    "    - num_samples: total number of points.\n",
    "    - num_features: dimension of the feature space.\n",
    "    - centers: number of blob clusters.\n",
    "    - cluster_std: standard deviation of each cluster.\n",
    "    - random_state: seed for reproducibility.\n",
    "    Returns: numpy array of shape (num_samples, num_features).\n",
    "    \"\"\"\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    # Choose random centers in [0,1] for each cluster\n",
    "    centroids = np.random.rand(centers, num_features) * 2.0 - 1.0  # in range [-1, 1] for variety\n",
    "    samples_per_center = num_samples // centers\n",
    "    data = []\n",
    "    for i in range(centers):\n",
    "        # generate samples around centroid\n",
    "        cov = (cluster_std ** 2) * np.eye(num_features)\n",
    "        points = np.random.multivariate_normal(mean=centroids[i], cov=cov, size=samples_per_center)\n",
    "        data.append(points)\n",
    "    data = np.vstack(data)\n",
    "    # If num_samples not exactly divisible by centers, generate remaining for the last center\n",
    "    if data.shape[0] < num_samples:\n",
    "        remaining = num_samples - data.shape[0]\n",
    "        points = np.random.multivariate_normal(mean=centroids[-1], cov=(cluster_std ** 2) * np.eye(num_features), size=remaining)\n",
    "        data = np.vstack([data, points])\n",
    "    np.random.shuffle(data)\n",
    "    return data\n",
    "\n",
    "def generate_spiral(num_samples: int = 500, revolutions: int = 3, noise: float = 0.0):\n",
    "    \"\"\"\n",
    "    Generate a 3D spiral curve dataset.\n",
    "    - num_samples: number of points along the spiral.\n",
    "    - revolutions: how many turns the spiral makes.\n",
    "    - noise: standard deviation of Gaussian noise to add to the points.\n",
    "    Returns: numpy array of shape (num_samples, 3).\n",
    "    \"\"\"\n",
    "    np.random.seed(None)\n",
    "    theta = np.linspace(0, 2 * np.pi * revolutions, num_samples)\n",
    "    z = np.linspace(0, 1, num_samples)  # height from 0 to 1\n",
    "    x = np.cos(theta)\n",
    "    y = np.sin(theta)\n",
    "    data = np.stack([x, y, z], axis=1)\n",
    "    if noise > 0:\n",
    "        data += np.random.normal(scale=noise, size=data.shape)\n",
    "    return data\n",
    "\n",
    "def generate_ring(num_samples: int = 500, radius: float = 1.0, noise: float = 0.01):\n",
    "    \"\"\"\n",
    "    Generate points in a ring (circle in 2D).\n",
    "    - num_samples: number of points on the ring.\n",
    "    - radius: radius of the ring.\n",
    "    - noise: standard deviation of radial noise.\n",
    "    Returns: numpy array of shape (num_samples, 2).\n",
    "    \"\"\"\n",
    "    np.random.seed(None)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_samples, endpoint=False)\n",
    "    x = radius * np.cos(angles)\n",
    "    y = radius * np.sin(angles)\n",
    "    data = np.stack([x, y], axis=1)\n",
    "    if noise > 0:\n",
    "        # Add noise by perturbing radius slightly\n",
    "        radii_noise = np.random.normal(scale=noise, size=num_samples)\n",
    "        data *= (1 + radii_noise)[:, None]\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-03T18:25:19.977110800Z",
     "start_time": "2025-06-03T18:25:19.965113900Z"
    }
   },
   "id": "230581e12e206583"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test Run"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a4d0618d9445cf95"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized SOOMap with 100 nodes, input_dim=2, device=cuda.\n",
      "Epoch 1: convergence (embedding) = 0.0%                     \n",
      "Epoch 2: convergence (embedding) = 0.0%                     \n",
      "Epoch 3: convergence (embedding) = 0.0%                     \n",
      "Epoch 4: convergence (embedding) = 0.0%                     \n",
      "Epoch 5: convergence (embedding) = 0.0%                     \n",
      "Epoch 6: convergence (embedding) = 0.0%                     \n",
      "Epoch 7: convergence (embedding) = 0.0%                     \n",
      "Epoch 8:  11%|â–ˆ         | 108/1000 [00:00<00:03, 268.59it/s]"
     ]
    }
   ],
   "source": [
    "# Generate a simple dataset: 2 clusters in 2D\n",
    "data_np = generate_blobs(num_samples=1000, num_features=2, centers=2, cluster_std=0.1, random_state=42)\n",
    "data_tensor = torch.tensor(data_np, dtype=torch.float32)\n",
    "\n",
    "# Instantiate the SOOMap with, say, 50 neurons, input_dim=2\n",
    "soo = SOOMap(num_nodes=100, input_dim=2, \n",
    "             init_weight_scale=0.01, init_coord_range=1.0, \n",
    "             learning_rate=0.2, neighbor_radius=0.5,\n",
    "             device=torch.device(\"cuda\"))\n",
    "\n",
    "# (Optional) Visualize initial node positions\n",
    "vis = SOOVisualizer(soo_map=soo, save_frames=False)\n",
    "vis.update(epoch=0)  # plot initial state (if visualization is enabled)\n",
    "\n",
    "# Train the SOOMap on the dataset for a few epochs\n",
    "soo.train(data_tensor, epochs=20, batch_size=1, ref_sample_size=100, min_lr=0.02, min_radius=0.05)\n",
    "\n",
    "# After training, update visualization to final state\n",
    "vis.update(epoch=20)\n",
    "vis.show()  # Show the interactive plot (if running in an environment with GUI support)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-06-03T18:25:19.977110800Z"
    }
   },
   "id": "17658294fc2a1d0"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "40972d66c3ca6bdc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "soo",
   "language": "python",
   "display_name": "py3.10 SOO"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
